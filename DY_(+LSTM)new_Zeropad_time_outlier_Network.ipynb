{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DY_(+LSTM)new_Zeropad_time_outlier_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNI_mDIM9XHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !nvidia-smi "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WFb0xuD9Y4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DjgPczbdW8iR"
      },
      "source": [
        "**Initialize**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tSMBsNl9W8iS",
        "outputId": "718c28b8-dd45-41e4-a36d-58c7798a4c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "read_file=scipy.io.loadmat('0pad in Time_100subject_50data_150length_mag_new_data.mat')\n",
        "Input_train =np.array(read_file['mag_Input_data']) # total_Input_data\n",
        "Input_test =np.array(read_file['mag_Input_data2']) # total_Input_data2\n",
        "\n",
        "X_train = Input_train[:,0:150]\n",
        "Y_train = Input_train[:,-1]\n",
        "\n",
        "X_test = Input_test[:,0:150]\n",
        "Y_test = Input_test[:,-1]\n",
        "\n",
        "# number of class\n",
        "n_classes= 100\n",
        "# number of features\n",
        "n_features=X_train.shape[1]\n",
        "\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)\n",
        "\n",
        "print(n_features)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15000, 150) (15000,)\n",
            "(15000, 150) (15000,)\n",
            "150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ye7FyK4WW8iU"
      },
      "source": [
        "**Set Target Subject**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6X8IH3GQW8iV",
        "outputId": "76d641cb-bc6d-4ec1-f158-3685454c95fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "### Target ID: ?(1-128) \n",
        "### Don't use: 4, 15, 19, 26, 29, 36, 37, 38, 40, 41, 50, 57, 59, 61, 67, 72, 80, 90, 97, 108, 110, 111, 122\n",
        "\n",
        "ID = 1\n",
        "print('ID',ID)\n",
        "\n",
        "Y_train[np.where(Y_train!=ID)] = 0\n",
        "Y_test[np.where(Y_test!=ID)] = 0\n",
        "   "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ID 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1-HLSovmW8iY",
        "outputId": "992c5e84-bb9e-4c95-d2be-18f562802db1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(np.unique(Y_train, return_counts=True)) # Train Target만 1인지 확인\n",
        "print(np.unique(Y_test, return_counts=True)) # Test Target만 1인지 확인"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([0., 1.]), array([14850,   150]))\n",
            "(array([0., 1.]), array([14850,   150]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ya8ff6lCW8ia"
      },
      "source": [
        "**Onehot Encoding + Data spliting and suffle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PFnCDHMEW8ib",
        "outputId": "fcc8197c-074f-4462-890d-ef4209db28de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "enc = OneHotEncoder()\n",
        "# randomly shuffle data before training and testing\n",
        "\n",
        "randIndx = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(randIndx)\n",
        "\n",
        "randIndx2 = np.arange(X_test.shape[0])\n",
        "np.random.shuffle(randIndx2)\n",
        "\n",
        "# print(randIndx)\n",
        "# print(randIndx2)\n",
        "\n",
        "trainSamples=np.floor(X_train.shape[0]).astype(int)\n",
        "testSamples=np.floor(X_test.shape[0]).astype(int) \n",
        "\n",
        "print(trainSamples)\n",
        "print(testSamples)\n",
        "\n",
        "X_train = X_train[randIndx,:] #Trainset 섞기\n",
        "X_test = X_test[randIndx2,:] #Trainset 섞기\n",
        " \n",
        "Y_train =enc.fit_transform(Y_train[randIndx].reshape(-1,1)).toarray() #rand에 의해 섞인 one-hot encoding Target\n",
        "Y_test =enc.fit_transform(Y_test[randIndx2].reshape(-1,1)).toarray() #rand에 의해 섞인 one-hot encoding Target\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15000\n",
            "15000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XNyB4tl2W8id",
        "outputId": "197ab2e8-0b05-447f-9037-37f7a9f7c8dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(np.unique(Y_train[:,1], return_counts=True))\n",
        "print(np.unique(Y_test[:,1], return_counts=True))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([0., 1.]), array([14850,   150]))\n",
            "(array([0., 1.]), array([14850,   150]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OkKJcKG3W8if",
        "outputId": "c6bccefc-7026-4827-f934-3d90e39ca5a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# total number of bands/ channel of features\n",
        "nChannels=1\n",
        "\n",
        "train_data, train_target = np.reshape(X_train,[trainSamples,n_features,nChannels]), Y_train\n",
        "\n",
        "test_data, test_target = np.reshape(X_test, [testSamples,n_features,nChannels]), Y_test\n",
        "\n",
        "\n",
        "\n",
        "print(train_data.shape,train_target.shape)\n",
        "print(test_data.shape,test_target.shape)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15000, 150, 1) (15000, 2)\n",
            "(15000, 150, 1) (15000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5tFW6dyNW8ii",
        "outputId": "5c0039f4-8cef-4e73-c4ad-649792aa9ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# 데이터 train/test에서 어떻게 구성됐는지 확인\n",
        "\n",
        "unique, counts = np.unique(train_target[:,1], return_counts=True)\n",
        "print('Train set dictionary: ',dict(zip(unique, counts)))\n",
        "\n",
        "unique, counts = np.unique(test_target[:,1], return_counts=True)\n",
        "print('Test set dictionary: ',dict(zip(unique, counts)))\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set dictionary:  {0.0: 14850, 1.0: 150}\n",
            "Test set dictionary:  {0.0: 14850, 1.0: 150}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "poQE6U0PW8ik"
      },
      "source": [
        "**Network Structure**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lfwkVdYKW8il",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "\n",
        "        self.sess = sess\n",
        "\n",
        "        self.name = name\n",
        "\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "\n",
        "        with tf.variable_scope(self.name):\n",
        "\n",
        "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
        "\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "            #training인 경우 1, 아닌 경우 0\n",
        "    \n",
        "            # input place holders\n",
        "            self.X = tf.placeholder(tf.float32, [None, 150, 1]) #None 사이즈 아무거나 들어감\n",
        "            self.Y = tf.placeholder(tf.float32, [None, 2])\n",
        "            self.class_weights = tf.placeholder(tf.float32) # For weighted loss\n",
        "            self.learning_rate = tf.placeholder(tf.float32)\n",
        "            \n",
        "            # Convolutional Layer #1, length: 150x1 -> 91x50  ((I-F)/S) + 1 #len 80에서 크기 30 good\n",
        "            W1 = tf.Variable(tf.random_normal([60, 1, 50], stddev=0.01))\n",
        "            conv1 = tf.nn.conv1d(self.X, W1, stride=1, padding='VALID')\n",
        "            relu1 = tf.nn.relu(conv1) # max(0,x)\n",
        "            dropout1 = tf.layers.dropout(inputs=relu1, rate=0.5, training=self.training)\n",
        "            \n",
        "            # Convolutional Layer #2, length: 91x50 -> 32x70 #len 80에서 크기 30 good\n",
        "            W2 = tf.Variable(tf.random_normal([60, 50, 70], stddev=0.01))\n",
        "            conv2 = tf.nn.conv1d(dropout1, W2, stride=1, padding='VALID')\n",
        "            relu2 = tf.nn.relu(conv2)                                    \n",
        "            dropout2 = tf.layers.dropout(inputs=relu2, rate=0.5, training=self.training)\n",
        "            \n",
        "            # RNN, 32x70 -> 32x32\n",
        "            num_layers = 1\n",
        "            hidden = 32\n",
        "            \n",
        "            cell = []\n",
        "            for i in range(num_layers):\n",
        "                cell.append(tf.contrib.rnn.LSTMCell(hidden, state_is_tuple=True))\n",
        "            cell = tf.contrib.rnn.MultiRNNCell(cell,state_is_tuple=True)\n",
        "            \n",
        "            batch_size = tf.shape(dropout2)[0]\n",
        "            seq_length = tf.shape(dropout2)[1]            \n",
        "            \n",
        "            initial_state = cell.zero_state(batch_size, tf.float32) #batch_size = dropout2.shape[0], 데이터 갯수\n",
        "            outputs, _states = tf.nn.dynamic_rnn(cell, dropout2, initial_state=initial_state, dtype=tf.float32)\n",
        "\n",
        "            FC1 = tf.reshape(outputs, [-1, 32*hidden])           \n",
        "            FC_weight = tf.get_variable(\"FC_weight\", shape=[32*hidden, 2], initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "            logits = tf.matmul(FC1,FC_weight)  \n",
        "\n",
        "    \n",
        "        # define cost/loss & optimizer   +  tf.nn.l2_loss(W3)\n",
        "        reg_losses = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2)  + tf.nn.l2_loss(FC_weight)\n",
        "        reg_constant = 0.005  # Choose an appropriate one.\n",
        "    \n",
        "        self.cost = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(self.Y, logits, self.class_weights)) \n",
        "        self.cost = self.cost + reg_constant * reg_losses\n",
        "        \n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost) \n",
        "#         self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.cost)    \n",
        "        \n",
        "        self.sigmoid_out = tf.nn.sigmoid(logits) # logit 결과를 0과 1 사이로 squeeze\n",
        "        self.predict = tf.cast(self.sigmoid_out[:,1], tf.float32)\n",
        "                    \n",
        "        self.weight1 = W1\n",
        "        self.weight2 = W2\n",
        "        \n",
        "    def get_predict(self, x_test, training=False):          \n",
        "\n",
        "        return self.sess.run(self.predict, feed_dict={self.X: x_test, self.training: training})\n",
        "    \n",
        "\n",
        "    def get_cost(self, x_valid, y_valid, weights, training=False):\n",
        "        \n",
        "        return self.sess.run(self.cost, feed_dict={self.X: x_valid, \n",
        "                                                   self.Y: y_valid, self.class_weights: weights, self.training: training})\n",
        "\n",
        "\n",
        "    def train(self, x_data, y_data, weights, learning, training=True):\n",
        "\n",
        "        return self.sess.run(self.optimizer, feed_dict={\n",
        "             self.X: x_data, self.Y: y_data, self.class_weights: weights, self.learning_rate: learning, self.training: training})\n",
        "    \n",
        "    def plot_feature(self, x_data, y_data, training=False):\n",
        "        \n",
        "        return self.sess.run(self.weight1, feed_dict={self.X: x_data, self.Y: y_data, self.training: training})\n",
        "    \n",
        "    def plot_feature2(self, x_data, y_data, training=False):\n",
        "        \n",
        "        return self.sess.run(self.weight2, feed_dict={self.X: x_data, self.Y: y_data, self.training: training})\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KPhm2f5wW8in"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0fiBMXjzW8io",
        "outputId": "79515d20-67a3-495e-c16c-67f56c101e89",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "### hyper parameters\n",
        "\n",
        "learning_rate = 0.0001\n",
        "training_epochs = 60\n",
        "batch_size = 1500 # Mini-batch size: total: 15000(train)/15000(test) \n",
        "num_cv = 10 #  Cross validation fold 횟수\n",
        "bag_size = 13500  # train size와 같아야 함\n",
        "ear_epoch = 0\n",
        "\n",
        "\n",
        "valid_cost = np.zeros((training_epochs))\n",
        "train_cost = np.zeros((training_epochs))\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "\n",
        "models = []\n",
        "num_models = 3 # Bagging model 갯수\n",
        "\n",
        "for m in range(num_models):\n",
        "    print('Number of Model+Bagging {}'.format(m+1))\n",
        "    models.append(Model(sess, \"model\" + str(m)))    \n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#saver = tf.train.Saver()\n",
        "\n",
        "start_time = time.time() # time starts!\n",
        "\n",
        "print('Learning Started!')\n",
        "\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "         \n",
        "    total_batch = int(bag_size / batch_size)\n",
        "    valid_cost_cv = 0\n",
        "    \n",
        "    cv = KFold(n_splits=num_cv, shuffle=True)\n",
        "    for train_index, test_index in cv.split(train_data):\n",
        "        cv_train_data, cv_validate_data = train_data[train_index], train_data[test_index]\n",
        "        cv_train_target, cv_validate_target = train_target[train_index], train_target[test_index]    \n",
        "                    \n",
        "        for m_idx, m in enumerate(models):\n",
        "            bag_index = np.random.choice(len(cv_train_target), bag_size, replace=True) # Random sampling with replacement\n",
        "            #print(bag_index)\n",
        "            bag_cv_train_data = cv_train_data[bag_index,:]\n",
        "            bag_cv_train_target = cv_train_target[bag_index,:] \n",
        "\n",
        "            for i in range(total_batch):\n",
        "\n",
        "                batch_xs, batch_ys = bag_cv_train_data[batch_size*i:batch_size*(i+1),:], bag_cv_train_target[batch_size*i:batch_size*(i+1)]                \n",
        "                pos_weight = (n_classes - 1) # Increase Sensitivity(FN down) and Decrease Specifity (FP up)\n",
        "#                 pos_weight = 1 # No weighted loss\n",
        "\n",
        "                _ = m.train(batch_xs, batch_ys, pos_weight, learning_rate)    \n",
        "                \n",
        "                pos_weight = 1\n",
        "#                 train_cost[epoch] += m.get_cost(batch_xs, batch_ys, pos_weight) / (num_cv * num_models * total_batch) #train_cost\n",
        "            \n",
        "            \n",
        "            valid_cost_cv += m.get_cost(cv_validate_data, cv_validate_target, pos_weight)  #valid_cost \n",
        "            \n",
        "    valid_cost[epoch] = valid_cost_cv / (num_cv * num_models) # valid_cost for cross validation\n",
        "    \n",
        "    if ((epoch+1)%5) == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'Validating_cost =', '{:.5f}'.format(valid_cost[epoch]))\n",
        "#         print('Epoch:', '%04d' % (epoch + 1), 'Training_cost =', '{:.5f}'.format(train_cost[epoch]))\n",
        "           \n",
        "            \n",
        "    if valid_cost[epoch] < 0.01:\n",
        "        print('Early Stopping, Epoch: ', epoch+1)\n",
        "        ear_epoch = epoch+1\n",
        "        break\n",
        "        \n",
        "        \n",
        "print('Learning Finished!')\n",
        "\n",
        "\n",
        "end_time = time.time() # time ends!\n",
        "\n",
        "elapsed = end_time - start_time\n",
        "\n",
        "print('Execution time:{:.2f} seconds'.format(elapsed))\n",
        "print('Last validation error:{:.3f} '.format(valid_cost[-1]))\n",
        "#saver.save(sess, 'SingleSession\\Model\\m'+str(ID)) # Model Save\n",
        "\n",
        "\n",
        "pos_weight = 1 # No weighted loss because no more training"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Model+Bagging 1\n",
            "WARNING:tensorflow:From <ipython-input-10-0e1956261091>:30: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-10-0e1956261091>:44: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-10-0e1956261091>:45: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-10-0e1956261091>:51: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Number of Model+Bagging 2\n",
            "Number of Model+Bagging 3\n",
            "Learning Started!\n",
            "Epoch: 0005 Validating_cost = 0.46997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zS-VmgtoW8ir"
      },
      "source": [
        "**Epoch vs Cost plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ppEE4d7xW8is",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.subplot(111)\n",
        "\n",
        "if ear_epoch == 0: # No early stop\n",
        "    x_axis = np.linspace(1.0, training_epochs, num=training_epochs)\n",
        "#     ax.plot(x_axis, train_cost, 'b', label='Training')\n",
        "    ax.plot(x_axis, valid_cost,'b', label='Validation')\n",
        "else: # Yes early stop\n",
        "    x_axis = np.linspace(1.0, ear_epoch, num=ear_epoch)\n",
        "#     ax.plot(x_axis, train_cost[:ear_epoch], 'b', label='Training')\n",
        "    ax.plot(x_axis, valid_cost[:ear_epoch],'b', label='Validation')\n",
        "\n",
        "ax.legend()\n",
        "plt.title('Epoch vs Cost')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nLgbv-NxW8iu"
      },
      "source": [
        "**Final Result of train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AgUia0GxW8iu",
        "colab": {}
      },
      "source": [
        "thres = np.arange(0, 1.0001, 0.0001)\n",
        "thres_matrix_train = np.zeros((len(train_target),len(thres)))\n",
        "total_matrix_train = np.zeros((len(thres),4)) #acc,far,frr,recall(order of column)\n",
        "    \n",
        "test_logit2_temp = np.zeros((len(train_target), num_models))\n",
        "train_cost2 = 0\n",
        "\n",
        "for m3_idx, m3 in enumerate(models):\n",
        "    test_logit2_temp[:,m3_idx] =  m3.get_predict(train_data)\n",
        "\n",
        "    if test_logit2_temp[np.where(test_logit2_temp< 0)] < 0:\n",
        "        print('Error')\n",
        "    else:\n",
        "        print('Okay, No minus values')       \n",
        "\n",
        "    train_cost2 += m3.get_cost(train_data,train_target,pos_weight) / num_models\n",
        "\n",
        "print('Train set Cost:{:.3f}'.format(train_cost2))\n",
        "\n",
        "test_logit2 = np.mean(test_logit2_temp, axis=1)\n",
        "\n",
        "for j in range(len(thres)):\n",
        "    for i in range(len(train_target)):\n",
        "        if test_logit2[i] < thres[j]:\n",
        "            thres_matrix_train[i,j] = 0\n",
        "        else:\n",
        "            thres_matrix_train[i,j] = 1\n",
        "\n",
        "\n",
        "unique, counts = np.unique(train_target[:,1], return_counts=True)\n",
        "print('Train set dictionary: ',dict(zip(unique, counts)))\n",
        "\n",
        "for i in range(len(thres)):\n",
        "\n",
        "    equal_logit2 = np.equal(thres_matrix_train[:,i], train_target[:,1]) ##Train\n",
        "    unique2, True_pos_neg_train = np.unique(thres_matrix_train[np.where(equal_logit2==True),i], return_counts=True)\n",
        "    unique3, False_pos_neg_train = np.unique(thres_matrix_train[np.where(equal_logit2==False),i], return_counts=True) \n",
        "\n",
        "    if np.shape(True_pos_neg_train)==(1,):\n",
        "        if unique2[0] == 0:\n",
        "            True_pos_neg_train = [True_pos_neg_train[0], 0]\n",
        "        else:\n",
        "            True_pos_neg_train = [0, True_pos_neg_train[0]]\n",
        "\n",
        "    if np.shape(False_pos_neg_train)==(1,):\n",
        "        if unique3[0] == 0:\n",
        "            False_pos_neg_train = [False_pos_neg_train[0], 0]\n",
        "        else:\n",
        "            False_pos_neg_train = [0, False_pos_neg_train[0]]\n",
        "   \n",
        "    if np.shape(True_pos_neg_train)==(0,):\n",
        "        True_pos_neg_train = [0, 0]\n",
        "        \n",
        "    if np.shape(False_pos_neg_train)==(0,):\n",
        "        False_pos_neg_train = [0, 0]\n",
        " \n",
        "    Recall_train= True_pos_neg_train[1]/(False_pos_neg_train[0]+True_pos_neg_train[1])\n",
        "    Specific_train = True_pos_neg_train[0] /(True_pos_neg_train[0]+False_pos_neg_train[1])\n",
        "\n",
        "    ACC_train = (True_pos_neg_train[0]+True_pos_neg_train[1]) / (len(train_target))\n",
        "    FAR_train = 1 - Specific_train\n",
        "    FRR_train = 1 - Recall_train\n",
        "\n",
        "    total_matrix_train[i,:] = ACC_train, FAR_train, FRR_train, Recall_train\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WP2hEG9yW8iw",
        "colab": {}
      },
      "source": [
        "fig2 = plt.figure()\n",
        "ax = plt.subplot(111)\n",
        "ax.plot(thres, total_matrix_train[:,1], 'b', label='FAR')\n",
        "ax.plot(thres, total_matrix_train[:,2], 'r', label='FRR')\n",
        "ax.legend()\n",
        "plt.title('FAR vs FRR (Train)')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Err Rate')\n",
        "plt.show()\n",
        "\n",
        "EER_line = thres[::-1]\n",
        "fig3 = plt.figure()\n",
        "ax2 = plt.subplot(111)\n",
        "ax2.plot(total_matrix_train[:,1], total_matrix_train[:,3],'r',label='ROC')\n",
        "ax2.plot(thres, EER_line, 'b',label='EER Line')\n",
        "ax2.legend()\n",
        "plt.title('ROC Curve (Train)')\n",
        "plt.xlabel('FAR')\n",
        "plt.ylabel('TPR')\n",
        "plt.show()\n",
        "\n",
        "EER_loc_train = np.argmin(abs(total_matrix_train[:,1] - total_matrix_train[:,2]))\n",
        "# print('EER threshold: ', thres[EER_loc_train])\n",
        "print('EER: {:.3f} '.format((total_matrix_train[EER_loc_train,1]+total_matrix_train[EER_loc_train,2])/2))\n",
        "print('FRR at EER: {:.3f} '.format(total_matrix_train[EER_loc_train,2]))\n",
        "print('FAR at EER: {:.3f} '.format(total_matrix_train[EER_loc_train,1]))\n",
        "print('Accuracy at EER: {:.3f} '.format(total_matrix_train[EER_loc_train,0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y-LNtpEVW8iy"
      },
      "source": [
        "**Final Result of test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e2tflZjhW8iy",
        "colab": {}
      },
      "source": [
        "# thres = np.arange(0, 1.0001, 0.0001)\n",
        "\n",
        "start_time_test = time.time() # time starts!\n",
        "\n",
        "thres_matrix_test = np.zeros((len(test_target),len(thres)))\n",
        "total_matrix_test = np.zeros((len(thres),4)) #acc,far,frr,recall(order of column)\n",
        "    \n",
        "test_logit_temp = np.zeros((len(test_target), num_models))\n",
        "test_cost = 0\n",
        "\n",
        "for m4_idx, m4 in enumerate(models):\n",
        "    test_logit_temp[:,m4_idx] =  m4.get_predict(test_data)\n",
        "\n",
        "    if test_logit_temp[np.where(test_logit_temp< 0)] < 0:\n",
        "        print('Error')\n",
        "    else:\n",
        "        print('Okay, No minus values')       \n",
        "\n",
        "    test_cost += m4.get_cost(test_data,test_target,pos_weight) / num_models\n",
        "\n",
        "print('Test set Cost: {:.3f}'.format(test_cost))\n",
        "\n",
        "test_logit = np.mean(test_logit_temp, axis=1)\n",
        "\n",
        "for j in range(len(thres)):\n",
        "    for i in range(len(test_target)):\n",
        "        if test_logit[i] < thres[j]:\n",
        "            thres_matrix_test[i,j] = 0\n",
        "        else:\n",
        "            thres_matrix_test[i,j] = 1\n",
        "\n",
        "\n",
        "unique, counts = np.unique(test_target[:,1], return_counts=True)\n",
        "print('Test set dictionary: ',dict(zip(unique, counts)))\n",
        "\n",
        "\n",
        "for i in range(len(thres)):\n",
        "\n",
        "    equal_logit = np.equal(thres_matrix_test[:,i], test_target[:,1]) ##Test\n",
        "    unique2, True_pos_neg_test = np.unique(thres_matrix_test[np.where(equal_logit==True),i], return_counts=True)\n",
        "    unique3, False_pos_neg_test = np.unique(thres_matrix_test[np.where(equal_logit==False),i], return_counts=True) \n",
        "\n",
        "    if np.shape(True_pos_neg_test)==(1,):\n",
        "        if unique2[0] == 0:\n",
        "            True_pos_neg_test = [True_pos_neg_test[0], 0]\n",
        "        else:\n",
        "            True_pos_neg_test = [0, True_pos_neg_test[0]]\n",
        "\n",
        "    if np.shape(False_pos_neg_test)==(1,):\n",
        "        if unique3[0] == 0:\n",
        "            False_pos_neg_test = [False_pos_neg_test[0], 0]\n",
        "        else:\n",
        "            False_pos_neg_test = [0, False_pos_neg_test[0]]\n",
        "   \n",
        "    if np.shape(True_pos_neg_test)==(0,):\n",
        "        True_pos_neg_test = [0, 0]\n",
        "        \n",
        "    if np.shape(False_pos_neg_test)==(0,):\n",
        "        False_pos_neg_test = [0, 0]\n",
        " \n",
        "    Recall_test= True_pos_neg_test[1]/(False_pos_neg_test[0]+True_pos_neg_test[1])\n",
        "    Specific_test = True_pos_neg_test[0] /(True_pos_neg_test[0]+False_pos_neg_test[1])\n",
        "\n",
        "    ACC_test = (True_pos_neg_test[0]+True_pos_neg_test[1]) / (len(test_target))\n",
        "    FAR_test = 1 - Specific_test\n",
        "    FRR_test = 1 - Recall_test\n",
        "\n",
        "    total_matrix_test[i,:] = ACC_test, FAR_test, FRR_test, Recall_test\n",
        "    \n",
        "end_time_test = time.time() # time ends! \n",
        "\n",
        "elapsed = end_time_test - start_time_test\n",
        "\n",
        "print('Execution Test time:{:.2f} seconds'.format(elapsed))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jY_BfxupW8i0",
        "colab": {}
      },
      "source": [
        "fig4 = plt.figure()\n",
        "ax = plt.subplot(111)\n",
        "ax.plot(thres, total_matrix_test[:,1], 'b', label='FAR')\n",
        "ax.plot(thres, total_matrix_test[:,2],'r', label='FRR')\n",
        "ax.legend()\n",
        "plt.title('FAR vs FRR (Test)')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Err Rate')\n",
        "plt.show()\n",
        "\n",
        "EER_line = thres[::-1]\n",
        "fig5 = plt.figure()\n",
        "ax2 = plt.subplot(111)\n",
        "ax2.plot(total_matrix_test[:,1], total_matrix_test[:,3],'r',label='ROC')\n",
        "ax2.plot(thres, EER_line, 'b',label='EER Line')\n",
        "ax2.legend()\n",
        "plt.title('ROC Curve (Test)')\n",
        "plt.xlabel('FAR')\n",
        "plt.ylabel('TPR')\n",
        "plt.show()\n",
        "\n",
        "EER_loc_test = np.argmin(abs(total_matrix_test[:,1] - total_matrix_test[:,2]))\n",
        "# print('EER threshold: ', thres[EER_loc_test])\n",
        "print('EER: {:.3f} '.format((total_matrix_test[EER_loc_test,1]+total_matrix_test[EER_loc_test,2])/2))\n",
        "print('FRR at EER: {:.3f} '.format(total_matrix_test[EER_loc_test,2]))\n",
        "print('FAR at EER: {:.3f} '.format(total_matrix_test[EER_loc_test,1]))\n",
        "print('Accuracy at EER: {:.3f} '.format(total_matrix_test[EER_loc_test,0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GWPbq9iHW8i1",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "fig11= plt.figure(figsize=(10,10))\n",
        "for m4_idx, m4 in enumerate(models):\n",
        "    ax = fig11.add_subplot(num_models,1,m4_idx+1)\n",
        "    plot_1 = m4.plot_feature(train_data, train_target)\n",
        "    ax.plot(np.mean(plot_1, axis=2))\n",
        "    plt.title('Filter1_Model'+str(m4_idx+1))\n",
        "plt.tight_layout()\n",
        " \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MRqiqQ1yW8i3",
        "colab": {}
      },
      "source": [
        "fig22= plt.figure(figsize=(10,10))\n",
        "for m4_idx, m4 in enumerate(models):\n",
        "    ax = fig22.add_subplot(num_models,1,m4_idx+1)\n",
        "    plot2 = m4.plot_feature2(train_data, train_target)\n",
        "    ax.plot(np.mean(np.mean(plot2, axis=2),axis=1))\n",
        "    plt.title('Filter2_Model'+str(m4_idx+1))\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-dM7f3ZYryq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('time_lstm_'+str(ID))\n",
        "\n",
        "fig.savefig('time_lstm_'+str(ID)+'/Epoch vs Cost_time_lstm_' +str(ID) +'.png', bbox_inches='tight')\n",
        "fig2.savefig('time_lstm_'+str(ID)+'/FAR vs FRR (Train)_time_lstm_' + str(ID) +'.png', bbox_inches='tight')\n",
        "fig3.savefig('time_lstm_'+str(ID)+'/ROC Curve (Train)_time_lstm_' + str(ID) +'.png', bbox_inches='tight')\n",
        "fig4.savefig('time_lstm_'+str(ID)+'/FAR vs FRR (Test)_time_lstm_' + str(ID) +'.png', bbox_inches='tight')\n",
        "fig5.savefig('time_lstm_'+str(ID)+'/ROC Curve (Test)_time_lstm_' + str(ID) +'.png', bbox_inches='tight')\n",
        "\n",
        "fig11.savefig('time_lstm_'+str(ID)+'/Avg_weight1_time_lstm_' + str(ID) +'.png', bbox_inches='tight')\n",
        "fig22.savefig('time_lstm_'+str(ID)+'/Avg_weight2_time_lstm_' + str(ID) +'.png', bbox_inches='tight')\n",
        "\n",
        "\n",
        "\n",
        "np.save('Total_Result_train_lstm_'+ str(ID)+'.npy',total_matrix_train)\n",
        "np.save('Total_Result_test_lstm_'+ str(ID)+'.npy',total_matrix_test)\n",
        "\n",
        "\n",
        "######### 여기 수동으로 변경 필요!!!!! (ID) ######################\n",
        "!zip -r /content/time_lstm_1.zip time_lstm_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaXDxhi3Yr7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('Total_Result_train_lstm_'+ str(ID)+'.npy')\n",
        "files.download('Total_Result_test_lstm_'+ str(ID)+'.npy')\n",
        "\n",
        "files.download('/content/time_lstm_'+str(ID)+'.zip')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHu9FlztYr_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.rmtree('time_lstm_'+str(ID))\n",
        "os.remove('time_lstm_'+str(ID)+'.zip')\n",
        "os.remove('Total_Result_train_lstm_'+ str(ID)+'.npy')\n",
        "os.remove('Total_Result_test_lstm_'+ str(ID)+'.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}